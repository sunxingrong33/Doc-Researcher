# Doc-Researcher with LLM API - ä½¿ç”¨è¯´æ˜

## ğŸ“‹ æ–‡ä»¶è¯´æ˜

### æ ¸å¿ƒæ–‡ä»¶

1. **llm_client.py** - LLMå®¢æˆ·ç«¯å°è£…
   - å°è£…äº†Qwen3 APIè°ƒç”¨
   - æä¾›å„ç§ä¸“ä¸šåŠŸèƒ½ï¼ˆè¡¨æ ¼æè¿°ã€æ‘˜è¦ç”Ÿæˆã€æ„å›¾åˆ†æç­‰ï¼‰
   - åŒ…å«é‡è¯•å’Œé”™è¯¯å¤„ç†é€»è¾‘

2. **doc_researcher_with_llm.py** - é›†æˆLLMçš„å®Œæ•´ç³»ç»Ÿ
   - ç»§æ‰¿åŸºç¡€Doc-ResearcheråŠŸèƒ½
   - æ‰€æœ‰éœ€è¦LLMçš„åœ°æ–¹éƒ½ä½¿ç”¨çœŸå®APIè°ƒç”¨
   - åŒ…å«å®Œæ•´çš„æ¼”ç¤ºä»£ç 

3. **test_llm_api.py** - APIæµ‹è¯•å·¥å…·
   - æµ‹è¯•LLMè¿æ¥
   - æµ‹è¯•å„ç§åŠŸèƒ½
   - éªŒè¯APIæ˜¯å¦æ­£å¸¸å·¥ä½œ

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ­¥éª¤1: æµ‹è¯•APIè¿æ¥

```bash
# æµ‹è¯•åŸºæœ¬è¿æ¥
python test_llm_api.py basic

# è¿è¡Œå®Œæ•´æµ‹è¯•
python test_llm_api.py
```

### æ­¥éª¤2: ä½¿ç”¨LLMå®¢æˆ·ç«¯

```python
from llm_client import LLMClient

# åˆ›å»ºå®¢æˆ·ç«¯
client = LLMClient(
    api_url="http://122.115.55.3:32800/v1/chat/completions",
    model="Qwen3_2507"
)

# ç®€å•å¯¹è¯
response = client.chat([
    {"role": "user", "content": "ä½ å¥½"}
])
print(response)

# ç”Ÿæˆè¡¨æ ¼æè¿°
table_md = "| åˆ—1 | åˆ—2 |\n|-----|-----|\n| A   | B   |"
description = client.generate_table_description(table_md)

# ç”Ÿæˆæ‘˜è¦
summary = client.generate_summary("é•¿æ–‡æœ¬...")

# åˆ†ææŸ¥è¯¢æ„å›¾
intent = client.analyze_query_intent("æŸ¥è¯¢å†…å®¹")

# ç”Ÿæˆå­æŸ¥è¯¢
subqueries = client.generate_subqueries("å¤æ‚æŸ¥è¯¢")
```

### æ­¥éª¤3: ä½¿ç”¨å®Œæ•´ç³»ç»Ÿ

```python
from doc_researcher_with_llm import LLMDocResearcher, LLMClient

# åˆ›å»ºLLMå®¢æˆ·ç«¯
llm_client = LLMClient()

# åˆ›å»ºDoc-Researcherç³»ç»Ÿ
researcher = LLMDocResearcher(
    llm_client=llm_client,
    max_iterations=3,
    sufficiency_threshold=0.7
)

# æ·»åŠ æ–‡æ¡£
documents = ["paper1.pdf", "report.pdf"]
researcher.add_documents(documents)

# æ‰§è¡Œç ”ç©¶
report = researcher.research("è¿™äº›æ–‡æ¡£çš„ä¸»è¦å‘ç°æ˜¯ä»€ä¹ˆï¼Ÿ")
print(report)

# å¤šè½®å¯¹è¯
report2 = researcher.research("èƒ½è¯¦ç»†è§£é‡Šä¸€ä¸‹ç¬¬ä¸€ä¸ªå‘ç°å—ï¼Ÿ")
print(report2)
```

## ğŸ”§ APIé…ç½®

### åŸºæœ¬é…ç½®

```python
client = LLMClient(
    api_url="http://122.115.55.3:32800/v1/chat/completions",  # APIåœ°å€
    model="Qwen3_2507",           # æ¨¡å‹åç§°
    timeout=1200,                 # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    max_retries=3                 # æœ€å¤§é‡è¯•æ¬¡æ•°
)
```

### è°ƒç”¨å‚æ•°

```python
response = client.chat(
    messages=[...],           # æ¶ˆæ¯åˆ—è¡¨
    temperature=0,            # æ¸©åº¦ (0-1, 0=ç¡®å®šæ€§)
    top_p=1,                  # top_pé‡‡æ ·
    max_tokens=4000,          # æœ€å¤§tokenæ•°
    system_prompt="..."       # ç³»ç»Ÿæç¤ºï¼ˆå¯é€‰ï¼‰
)
```

## ğŸ“Š LLMå®¢æˆ·ç«¯åŠŸèƒ½è¯¦è§£

### 1. åŸºç¡€å¯¹è¯

```python
client = LLMClient()

# ç®€å•é—®ç­”
response = client.chat([
    {"role": "user", "content": "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ"}
])

# å¸¦ç³»ç»Ÿæç¤º
response = client.chat(
    messages=[{"role": "user", "content": "ä»‹ç»æ·±åº¦å­¦ä¹ "}],
    system_prompt="ä½ æ˜¯ä¸€ä½AIä¸“å®¶"
)
```

### 2. è¡¨æ ¼æè¿°ç”Ÿæˆ

```python
table_markdown = """
| æ¨¡å‹ | å‡†ç¡®ç‡ | é€Ÿåº¦ |
|------|--------|------|
| A    | 85%    | å¿«   |
| B    | 90%    | æ…¢   |
"""

description = client.generate_table_description(table_markdown)
# è¿”å›æ ¼å¼:
# [ç²—ç²’åº¦æè¿°]
# è¡¨æ ¼å¯¹æ¯”äº†ä¸¤ä¸ªæ¨¡å‹çš„æ€§èƒ½ã€‚
# [ç»†ç²’åº¦æè¿°]
# è¡¨æ ¼åŒ…å«3åˆ—ï¼šæ¨¡å‹ã€å‡†ç¡®ç‡ã€é€Ÿåº¦...
```

### 3. å›¾ç‰‡æè¿°ç”Ÿæˆ

```python
# åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆå›¾ç‰‡æè¿°
description = client.generate_figure_description(
    figure_context="å›¾1: ç³»ç»Ÿæ¶æ„å›¾"
)
```

### 4. æ‘˜è¦ç”Ÿæˆ

```python
long_text = """å¾ˆé•¿çš„æ–‡æ¡£å†…å®¹..."""

summary = client.generate_summary(
    full_text=long_text,
    max_length=200  # æ‘˜è¦æœ€å¤§é•¿åº¦
)
```

### 5. æŸ¥è¯¢æ„å›¾åˆ†æ

```python
query = "æ¯”è¾ƒBERTå’ŒGPTåœ¨æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸Šçš„è¡¨ç°"

intent = client.analyze_query_intent(query)
# è¿”å›:
# {
#     "intent_type": "comparison",
#     "granularity": "chunk",
#     "complexity": "medium",
#     "needs_multi_doc": true
# }
```

### 6. å­æŸ¥è¯¢ç”Ÿæˆ

```python
complex_query = "åˆ†ææ·±åº¦å­¦ä¹ åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨ã€æŒ‘æˆ˜å’Œæœªæ¥å‘å±•"

subqueries = client.generate_subqueries(
    query=complex_query,
    max_subqueries=3
)
# è¿”å›:
# [
#     "æ·±åº¦å­¦ä¹ åœ¨åŒ»ç–—é¢†åŸŸçš„å…·ä½“åº”ç”¨",
#     "æ·±åº¦å­¦ä¹ åœ¨åŒ»ç–—é¢†åŸŸé¢ä¸´çš„æŒ‘æˆ˜",
#     "æ·±åº¦å­¦ä¹ åœ¨åŒ»ç–—é¢†åŸŸçš„æœªæ¥å‘å±•è¶‹åŠ¿"
# ]
```

### 7. ä¿¡æ¯å……åˆ†æ€§è¯„ä¼°

```python
query = "æ·±åº¦å­¦ä¹ çš„ä¼˜åŠ¿"
retrieved_contents = [
    "æ·±åº¦å­¦ä¹ å¯ä»¥è‡ªåŠ¨å­¦ä¹ ç‰¹å¾",
    "æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«è¡¨ç°å‡ºè‰²"
]

sufficiency = client.evaluate_information_sufficiency(
    query=query,
    retrieved_contents=retrieved_contents
)
# è¿”å›: 0.0-1.0ä¹‹é—´çš„åˆ†æ•°
```

### 8. æŠ¥å‘Šç”Ÿæˆ

```python
query = "æ·±åº¦å­¦ä¹ çš„åº”ç”¨"
evidence_list = [
    {
        'content': 'æ·±åº¦å­¦ä¹ åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸåº”ç”¨å¹¿æ³›',
        'doc_id': 'doc1',
        'relevance': 0.9
    },
    # ...æ›´å¤šè¯æ®
]

report = client.generate_report(
    query=query,
    evidence_list=evidence_list,
    conversation_history=[...]  # å¯é€‰
)
```

## ğŸ—ï¸ å®Œæ•´ç³»ç»Ÿæ¶æ„

```
ç”¨æˆ·æŸ¥è¯¢
   â†“
LLMPlannerAgent (ä½¿ç”¨LLMåˆ†ææ„å›¾)
   â”œâ”€â”€ æ–‡æ¡£è¿‡æ»¤
   â”œâ”€â”€ ç²’åº¦é€‰æ‹© (åŸºäºLLMåˆ†æ)
   â””â”€â”€ å­æŸ¥è¯¢ç”Ÿæˆ (ä½¿ç”¨LLM)
   â†“
è¿­ä»£å¾ªç¯
   â”œâ”€â”€ SearcherAgent (æ£€ç´¢)
   â”œâ”€â”€ LLMRefinerAgent (ç²¾ç‚¼)
   â””â”€â”€ å……åˆ†æ€§è¯„ä¼° (ä½¿ç”¨LLM)
   â†“
LLMReporterAgent (ä½¿ç”¨LLMç”ŸæˆæŠ¥å‘Š)
   â†“
ç ”ç©¶æŠ¥å‘Š
```

## ğŸ’¡ ä½¿ç”¨æŠ€å·§

### 1. é”™è¯¯å¤„ç†

```python
from llm_client import LLMClient

try:
    client = LLMClient()
    response = client.chat([...])
except Exception as e:
    print(f"LLMè°ƒç”¨å¤±è´¥: {e}")
    # å¤„ç†é”™è¯¯æˆ–ä½¿ç”¨é»˜è®¤å€¼
```

### 2. è¶…æ—¶è®¾ç½®

```python
# å¯¹äºå¤æ‚ä»»åŠ¡ï¼Œå¢åŠ è¶…æ—¶æ—¶é—´
client = LLMClient(timeout=1800)  # 30åˆ†é’Ÿ

# å¯¹äºç®€å•ä»»åŠ¡ï¼Œå¯ä»¥ç¼©çŸ­
client = LLMClient(timeout=300)   # 5åˆ†é’Ÿ
```

### 3. æ¸©åº¦è°ƒèŠ‚

```python
# ç¡®å®šæ€§ä»»åŠ¡ï¼ˆæ‘˜è¦ã€åˆ†æï¼‰
response = client.chat(messages, temperature=0)

# åˆ›æ„æ€§ä»»åŠ¡ï¼ˆç”Ÿæˆæè¿°ï¼‰
response = client.chat(messages, temperature=0.7)
```

### 4. Tokené™åˆ¶

```python
# çŸ­å›ç­”
response = client.chat(messages, max_tokens=500)

# é•¿æŠ¥å‘Š
response = client.chat(messages, max_tokens=2000)
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### 1. æ‰¹é‡å¤„ç†

```python
# ä¸å¥½ - ä¸²è¡Œå¤„ç†
for doc in documents:
    summary = client.generate_summary(doc)

# å¥½ - æ‰¹é‡å‡†å¤‡åå¤„ç†
summaries = []
for doc in documents:
    summaries.append(client.generate_summary(doc))
```

### 2. ç¼“å­˜ç»“æœ

```python
import json

# ç¼“å­˜æ‘˜è¦
cache = {}
def get_summary_cached(text):
    text_hash = hash(text)
    if text_hash not in cache:
        cache[text_hash] = client.generate_summary(text)
    return cache[text_hash]
```

### 3. å¹¶è¡Œè°ƒç”¨

```python
from concurrent.futures import ThreadPoolExecutor

def process_document(doc):
    return client.generate_summary(doc)

with ThreadPoolExecutor(max_workers=3) as executor:
    summaries = list(executor.map(process_document, documents))
```

## ğŸ› å¸¸è§é—®é¢˜

### Q1: APIè¿æ¥å¤±è´¥

```
é”™è¯¯: requests.exceptions.ConnectionError
```

**è§£å†³æ–¹æ¡ˆ:**
1. æ£€æŸ¥APIåœ°å€æ˜¯å¦æ­£ç¡®
2. ç¡®è®¤ç½‘ç»œè¿æ¥
3. éªŒè¯APIæœåŠ¡æ˜¯å¦è¿è¡Œ
4. å°è¯•æ‰‹åŠ¨curlæµ‹è¯•

```bash
curl -X POST http://122.115.55.3:32800/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen3_2507",
    "messages": [{"role": "user", "content": "æµ‹è¯•"}],
    "max_tokens": 50
  }'
```

### Q2: è¶…æ—¶é”™è¯¯

```
é”™è¯¯: requests.exceptions.Timeout
```

**è§£å†³æ–¹æ¡ˆ:**
1. å¢åŠ timeoutå‚æ•°
2. å‡å°‘max_tokens
3. ç®€åŒ–æŸ¥è¯¢

### Q3: JSONè§£æé”™è¯¯

```
é”™è¯¯: json.JSONDecodeError
```

**è§£å†³æ–¹æ¡ˆ:**
1. æ£€æŸ¥APIè¿”å›æ ¼å¼
2. ä½¿ç”¨æ›´æ˜ç¡®çš„æç¤ºè¯
3. æ·»åŠ é¢å¤–çš„é”™è¯¯å¤„ç†

### Q4: å†…å®¹è¢«æˆªæ–­

**è§£å†³æ–¹æ¡ˆ:**
```python
# å¢åŠ max_tokens
response = client.chat(messages, max_tokens=4000)
```

## ğŸ“ æœ€ä½³å®è·µ

### 1. æç¤ºè¯å·¥ç¨‹

```python
# å¥½çš„æç¤ºè¯
system_prompt = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–‡æ¡£åˆ†æä¸“å®¶ã€‚
è¦æ±‚:
1. æä¾›å‡†ç¡®çš„åˆ†æ
2. ä½¿ç”¨ç»“æ„åŒ–æ ¼å¼
3. ä¿æŒå®¢è§‚ä¸­ç«‹"""

# ä¸å¥½çš„æç¤ºè¯
system_prompt = "åˆ†ææ–‡æ¡£"
```

### 2. é”™è¯¯æ¢å¤

```python
def safe_llm_call(func, *args, default=None, **kwargs):
    """å®‰å…¨çš„LLMè°ƒç”¨ï¼Œå¸¦é»˜è®¤å€¼"""
    try:
        return func(*args, **kwargs)
    except Exception as e:
        print(f"LLMè°ƒç”¨å¤±è´¥: {e}")
        return default

# ä½¿ç”¨
summary = safe_llm_call(
    client.generate_summary,
    text,
    default="æ‘˜è¦ç”Ÿæˆå¤±è´¥"
)
```

### 3. æ—¥å¿—è®°å½•

```python
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åœ¨LLMè°ƒç”¨å‰åè®°å½•
logger.info(f"è°ƒç”¨LLM: {query}")
response = client.chat(messages)
logger.info(f"LLMå“åº”é•¿åº¦: {len(response)}")
```

## ğŸ” è°ƒè¯•æŠ€å·§

### 1. æ‰“å°ä¸­é—´ç»“æœ

```python
# æŸ¥çœ‹LLMå®é™…å‘é€çš„å†…å®¹
print(f"å‘é€ç»™LLMçš„æ¶ˆæ¯: {messages}")

# æŸ¥çœ‹åŸå§‹å“åº”
response = client.chat(messages)
print(f"LLMåŸå§‹å“åº”: {response}")
```

### 2. ä½¿ç”¨æµ‹è¯•æ¨¡å¼

```python
# åˆ›å»ºæµ‹è¯•å®¢æˆ·ç«¯
test_client = LLMClient(timeout=60)  # çŸ­è¶…æ—¶ç”¨äºæµ‹è¯•

# ç®€å•æµ‹è¯•
try:
    response = test_client.chat([
        {"role": "user", "content": "æµ‹è¯•"}
    ], max_tokens=10)
    print("âœ… APIæ­£å¸¸å·¥ä½œ")
except Exception as e:
    print(f"âŒ APIå¼‚å¸¸: {e}")
```

## ğŸ“š ç¤ºä¾‹ä»£ç 

å®Œæ•´çš„ä½¿ç”¨ç¤ºä¾‹è¯·å‚è€ƒ:
- `test_llm_api.py` - APIæµ‹è¯•ç¤ºä¾‹
- `doc_researcher_with_llm.py` - å®Œæ•´ç³»ç»Ÿç¤ºä¾‹

## ğŸ¯ ä¸‹ä¸€æ­¥

1. è¿è¡Œæµ‹è¯•éªŒè¯APIè¿æ¥
2. å°è¯•åŸºæœ¬çš„LLMè°ƒç”¨
3. ä½¿ç”¨å®Œæ•´çš„Doc-Researcherç³»ç»Ÿ
4. æ ¹æ®éœ€è¦è°ƒæ•´å‚æ•°å’Œæç¤ºè¯

---

**éœ€è¦å¸®åŠ©?** 
- æŸ¥çœ‹æµ‹è¯•è„šæœ¬è·å–æ›´å¤šç¤ºä¾‹
- æ£€æŸ¥APIæ–‡æ¡£äº†è§£æ›´å¤šå‚æ•°
- è°ƒæ•´æç¤ºè¯ä»¥è·å¾—æ›´å¥½çš„ç»“æœ
